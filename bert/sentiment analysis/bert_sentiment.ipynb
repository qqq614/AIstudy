{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acceptable-whale",
   "metadata": {},
   "source": [
    "### BERT with sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "strange-reynolds",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ref \n",
    "# english : https://skimai.com/fine-tuning-bert-for-sentiment-analysis/\n",
    "# korean : https://github.com/monologg/KoBERT-nsmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "engaging-german",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import math\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "import torch\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import random_split, TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import json, copy\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "played-winning",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-9389b6ac3cb0>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-9389b6ac3cb0>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    from transformers import\u001b[0m\n\u001b[0m                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertConfig, BertForSequenceClassification \n",
    "from tokenization_kobert import KoBertTokenizer\n",
    "pretrained_model_name='monologg/kobert'\n",
    "tokenizer = KoBertTokenizer.from_pretrained(pretrained_model_name)\n",
    "tokenizer.tokenize('무리뉴')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "specified-chain",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /kykim/bert-kor-base/resolve/main/vocab.txt HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /kykim/bert-kor-base/resolve/main/tokenizer.json HTTP/1.1\" 404 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /kykim/bert-kor-base/resolve/main/added_tokens.json HTTP/1.1\" 404 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /kykim/bert-kor-base/resolve/main/special_tokens_map.json HTTP/1.1\" 404 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /kykim/bert-kor-base/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['맛있는', '거', '먹구', '싶어']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertConfig, BertForSequenceClassification \n",
    "from transformers import BertTokenizerFast\n",
    "pretrained_model_name='kykim/bert-kor-base'\n",
    "tokenizer = BertTokenizerFast.from_pretrained(pretrained_model_name)\n",
    "tokenizer.tokenize('맛있는 거 먹구 싶어')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "revised-schema",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Set seed for reproducibility.\n",
    "    \"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "chief-graham",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make training dataset\n",
    "model_name=''\n",
    "cache_dir='./cache'\n",
    "model_dir='./model'\n",
    "version='model_v1'\n",
    "model_save_path=os.path.join(model_dir, version)\n",
    "data_dir='./data'\n",
    "train_file='ratings_train.txt'\n",
    "test_file='ratings_test.txt'\n",
    "max_seq_len=100\n",
    "# no weight decay, gradient accumulation steps\n",
    "\n",
    "train_batch_size=10\n",
    "dev_batch_size=32\n",
    "test_batch_size=32\n",
    "\n",
    "num_train_epochs=5\n",
    "learning_rate=5e-5\n",
    "max_grad_norm=1.0\n",
    "weight_decay=0.0\n",
    "save_steps=2000\n",
    "adam_epsilon=1e-8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "naked-thickness",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read files or read from cache and make train_dataset, test_dataset  and save cache\n",
    "\n",
    "cached_train_filename='cached_{}_{}'.format(pretrained_model_name.replace('/','-'), train_file.split('.')[0])\n",
    "cached_test_filename='cached_{}_{}'.format(pretrained_model_name.replace('/','-'), test_file.split('.')[0])\n",
    "cached_train_file=os.path.join(cache_dir, cached_train_filename)\n",
    "cached_test_file=os.path.join(cache_dir, cached_test_filename)\n",
    "train_data_file=os.path.join(data_dir, train_file)\n",
    "test_data_file=os.path.join(data_dir, test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extreme-bones",
   "metadata": {},
   "source": [
    "#### convert trainfile into features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "detected-telescope",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, attention_mask, token_type_ids, label_id):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_mask = attention_mask\n",
    "        self.token_type_ids = token_type_ids\n",
    "        self.label_id = label_id\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.to_json_string())\n",
    "\n",
    "    def to_dict(self):\n",
    "        \"\"\"Serializes this instance to a Python dictionary.\"\"\"\n",
    "        output = copy.deepcopy(self.__dict__)\n",
    "        return output\n",
    "\n",
    "    def to_json_string(self):\n",
    "        \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
    "        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "contained-swimming",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loading features from cached file ./cache/cached_kykim-bert-kor-base_ratings_train\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(cached_train_file):\n",
    "    logger.info(\"Loading features from cached file %s\", cached_train_file)\n",
    "    features=torch.load(cached_train_file)\n",
    "else:\n",
    "    logger.info(\"Creating features from dataset file at %s with tokenizer %s\", data_dir, model_name)\n",
    "    lines=[]\n",
    "    with open(train_data_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            lines.append(line.strip())\n",
    "    examples=[]\n",
    "    for i, line in enumerate(lines[1:]):\n",
    "        splits=line.split('\\t')\n",
    "        assert len(splits)>2, 'wrong length at {}th line {}'.format(i, line)\n",
    "        examples.append(('train_'+str(i), splits[1], int(splits[2])))\n",
    "        if i%5000==0:\n",
    "            logger.info(line)\n",
    "\n",
    "    # features = convert_into_features(cached_train_file,max_seq_len, tokenizer)\n",
    "\n",
    "\n",
    "    cls_token=tokenizer.cls_token\n",
    "    sep_token=tokenizer.sep_token\n",
    "    pad_token_id=tokenizer.pad_token_id\n",
    "\n",
    "    features=[]\n",
    "    for (ex_idx, example) in enumerate(examples):\n",
    "        if ex_idx % 10000 == 0:\n",
    "            logger.info(\"Writing example %d of %d\" % (ex_idx, len(examples)))\n",
    "        tokens=tokenizer.tokenize(example[1])\n",
    "\n",
    "        special_tokens_count = 2\n",
    "        if len(tokens) > max_seq_len - special_tokens_count:\n",
    "            tokens = tokens[:(max_seq_len - special_tokens_count)]\n",
    "\n",
    "\n",
    "        tokens=[cls_token]+tokens+[sep_token]\n",
    "        token_type_ids=[0]*len(tokens)\n",
    "\n",
    "        input_ids=tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        attention_mask=[1]*len(input_ids)\n",
    "\n",
    "        padding_length=max_seq_len-len(input_ids)\n",
    "\n",
    "        input_ids = input_ids + ([pad_token_id] * padding_length)\n",
    "        attention_mask = attention_mask + ([0] * padding_length)\n",
    "        token_type_ids = token_type_ids + ([0] * padding_length)\n",
    "        features.append(InputFeatures(input_ids, attention_mask, token_type_ids, example[2]))\n",
    "        if ex_idx < 5:\n",
    "            logger.info(\"*** Example ***\")\n",
    "            logger.info(\"idx: %s\" % example[0])\n",
    "            logger.info(\"tokens: %s\" % \" \".join([str(x) for x in tokens]))\n",
    "            logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "            logger.info(\"attention_mask: %s\" % \" \".join([str(x) for x in attention_mask]))\n",
    "            logger.info(\"token_type_ids: %s\" % \" \".join([str(x) for x in token_type_ids]))\n",
    "            logger.info(\"label: %s\" % example[2])\n",
    "        \n",
    "    logger.info(\"Saving features into cached train file %s\", cached_train_file)\n",
    "    torch.save(features, cached_train_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graduate-bacon",
   "metadata": {},
   "source": [
    "#### set seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "unique-collective",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "integrated-attachment",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "all_attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.long)\n",
    "all_token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.long)\n",
    "all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.long)\n",
    "\n",
    "train_dev_dataset = TensorDataset(all_input_ids, all_attention_mask,\n",
    "                        all_token_type_ids, all_label_ids)\n",
    "\n",
    "sample_rate=0.01\n",
    "dev_size = math.ceil(0.2*len(train_dev_dataset))\n",
    "train_size=len(train_dev_dataset)-dev_size\n",
    "\n",
    "train_dataset, dev_dataset = random_split(train_dev_dataset, [train_size, dev_size])\n",
    "# dev_size=math.ceil(sample_rate*dev_size)\n",
    "# train_size=math.ceil(sample_rate*train_size)\n",
    "# rest_size=len(train_dev_dataset)-train_size-dev_size\n",
    "# train_dataset, dev_dataset, rest_dataset = random_split(train_dev_dataset, [train_size, dev_size, rest_size])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cellular-english",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000, 100])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_input_ids.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "leading-manchester",
   "metadata": {},
   "source": [
    "#### convert testfile into features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "committed-virgin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loading features from cached test file ./cache/cached_kykim-bert-kor-base_ratings_test\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(cached_test_file):\n",
    "    logger.info(\"Loading features from cached test file %s\", cached_test_file)\n",
    "    features=torch.load(cached_test_file)\n",
    "else: \n",
    "    logger.info(\"Creating features from dataset file at %s with tokenizer %s\", data_dir, model_name)\n",
    "    lines=[]\n",
    "    with open(test_data_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            lines.append(line.strip())\n",
    "    examples=[]\n",
    "    for i, line in enumerate(lines[1:]):\n",
    "        splits=line.split('\\t')\n",
    "        assert len(splits)>2, 'wrong length at {}th line {}'.format(i, line)\n",
    "        examples.append(('test_'+str(i), splits[1], int(splits[2])))\n",
    "        if i%5000==0:\n",
    "            logger.info(line)\n",
    "    features=[]\n",
    "    for (ex_idx, example) in enumerate(examples):\n",
    "        if ex_idx % 10000 == 0:\n",
    "            logger.info(\"Writing example %d of %d\" % (ex_idx, len(examples)))\n",
    "        tokens=tokenizer.tokenize(example[1])\n",
    "\n",
    "        special_tokens_count = 2\n",
    "        if len(tokens) > max_seq_len - special_tokens_count:\n",
    "            tokens = tokens[:(max_seq_len - special_tokens_count)]\n",
    "\n",
    "\n",
    "        tokens=[cls_token]+tokens+[sep_token]\n",
    "        token_type_ids=[0]*len(tokens)\n",
    "\n",
    "        input_ids=tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        attention_mask=[1]*len(input_ids)\n",
    "\n",
    "        padding_length=max_seq_len-len(input_ids)\n",
    "\n",
    "        input_ids = input_ids + ([pad_token_id] * padding_length)\n",
    "        attention_mask = attention_mask + ([0] * padding_length)\n",
    "        token_type_ids = token_type_ids + ([0] * padding_length)\n",
    "        features.append(InputFeatures(input_ids, attention_mask, token_type_ids, example[2]))\n",
    "        \n",
    "        if ex_idx < 5:\n",
    "            logger.info(\"*** Example ***\")\n",
    "            logger.info(\"idx: %s\" % example[0])\n",
    "            logger.info(\"tokens: %s\" % \" \".join([str(x) for x in tokens]))\n",
    "            logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "            logger.info(\"attention_mask: %s\" % \" \".join([str(x) for x in attention_mask]))\n",
    "            logger.info(\"token_type_ids: %s\" % \" \".join([str(x) for x in token_type_ids]))\n",
    "            logger.info(\"label: %s\" % example[2])\n",
    "        \n",
    "    logger.info(\"Saving features into cached test file %s\", cached_test_file)\n",
    "    torch.save(features, cached_test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "behavioral-warning",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "all_input_ids=torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "all_attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.long)\n",
    "all_token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.long)\n",
    "all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.long)\n",
    "\n",
    "test_dataset = TensorDataset(all_input_ids, all_attention_mask,\n",
    "                        all_token_type_ids, all_label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "written-preview",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000, 100])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_input_ids.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "australian-pollution",
   "metadata": {},
   "source": [
    "#### model loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "future-vampire",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list=[0,1]\n",
    "id2label={str(i):label for i, label in enumerate(label_list)}\n",
    "label2id={label:i for i, label in enumerate(label_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "respective-receptor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /kykim/bert-kor-base/resolve/main/config.json HTTP/1.1\" 200 0\n"
     ]
    }
   ],
   "source": [
    "model_config=BertConfig.from_pretrained(pretrained_model_name,\n",
    "                                       num_labels=len(label_list),\n",
    "                                       finetuning_task='nsmc',\n",
    "                                       id2label=id2label,\n",
    "                                       label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "statewide-campus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /kykim/bert-kor-base/resolve/main/pytorch_model.bin HTTP/1.1\" 302 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kykim/bert-kor-base were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at kykim/bert-kor-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model=BertForSequenceClassification.from_pretrained(pretrained_model_name, config=model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "consolidated-federal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(42000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pregnant-newark",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "controlled-shoulder",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler=RandomSampler(train_dataset)\n",
    "train_dataloader=DataLoader(train_dataset, sampler=train_sampler, batch_size=train_batch_size)\n",
    "\n",
    "dev_sampler=SequentialSampler(dev_dataset)\n",
    "dev_dataloader=DataLoader(dev_dataset, sampler=dev_sampler, batch_size=dev_batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ancient-storm",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "enable_weight_decay=False\n",
    "t_total=len(train_dataloader)\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay': weight_decay},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer=AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=adam_epsilon)\n",
    "#     optimizer=AdamW(model.parameters(), lr=learning_rate, eps=adam_epsilon)\n",
    "scheduler=get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=t_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "secondary-electron",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_param=[(n,p) for n, p in model.named_parameters() if 'classifier' in n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "optimum-infrared",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('classifier.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0033, -0.0223,  0.0195,  ..., -0.0075,  0.0172,  0.0187],\n",
       "          [ 0.0212, -0.0007,  0.0149,  ..., -0.0155, -0.0101, -0.0068]],\n",
       "         device='cuda:0', requires_grad=True)),\n",
       " ('classifier.bias',\n",
       "  Parameter containing:\n",
       "  tensor([0., 0.], device='cuda:0', requires_grad=True))]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "institutional-patch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:__main__:num examples 120000\n",
      "INFO:__main__:train batch size 10\n",
      "INFO:__main__:num of train epochs : 5\n",
      "INFO:__main__:num of train steps each epoch : 12000\n",
      "INFO:__main__:epoch : 0\n",
      "INFO:__main__:train mode\n",
      "INFO:__main__:train_step : 1000, elapsed time[420.3530158996582]\n",
      "INFO:__main__:train_step : 2000, elapsed time[843.7727801799774]\n",
      "INFO:__main__:train_step : 3000, elapsed time[1267.1378231048584]\n",
      "INFO:__main__:train_step : 4000, elapsed time[1690.947556734085]\n",
      "INFO:__main__:train_step : 5000, elapsed time[2114.791724920273]\n",
      "INFO:__main__:train_step : 6000, elapsed time[2538.8696088790894]\n",
      "INFO:__main__:train_step : 7000, elapsed time[2963.0130043029785]\n",
      "INFO:__main__:train_step : 8000, elapsed time[3386.9477636814117]\n",
      "INFO:__main__:train_step : 9000, elapsed time[3810.881938457489]\n",
      "INFO:__main__:train_step : 10000, elapsed time[4234.661637067795]\n",
      "INFO:__main__:train_step : 11000, elapsed time[4658.297646045685]\n",
      "INFO:__main__:train_step : 12000, elapsed time[5082.092885971069]\n",
      "INFO:__main__:train_loss : 0.3796031404286623\n",
      "INFO:__main__:eval mode\n",
      "INFO:__main__:eval loss : 0.301277593036355, acc : 0.8843333333333333\n",
      "INFO:__main__:model saved to ./model\n",
      "INFO:__main__:epoch : 1\n",
      "INFO:__main__:train mode\n",
      "INFO:__main__:train_step : 1000, elapsed time[423.66611337661743]\n",
      "INFO:__main__:train_step : 2000, elapsed time[847.4588539600372]\n",
      "INFO:__main__:train_step : 3000, elapsed time[1271.3054325580597]\n",
      "INFO:__main__:train_step : 4000, elapsed time[1695.1587195396423]\n",
      "INFO:__main__:train_step : 5000, elapsed time[2119.26407456398]\n",
      "INFO:__main__:train_step : 6000, elapsed time[2543.3963820934296]\n",
      "INFO:__main__:train_step : 7000, elapsed time[2967.352229118347]\n",
      "INFO:__main__:train_step : 8000, elapsed time[3391.1917753219604]\n",
      "INFO:__main__:train_step : 9000, elapsed time[3815.2789273262024]\n",
      "INFO:__main__:train_step : 10000, elapsed time[4239.511121749878]\n",
      "INFO:__main__:train_step : 11000, elapsed time[4663.686125040054]\n",
      "INFO:__main__:train_step : 12000, elapsed time[5087.56267619133]\n",
      "INFO:__main__:train_loss : 0.2632183164213784\n",
      "INFO:__main__:eval mode\n",
      "INFO:__main__:eval loss : 0.301277593036355, acc : 0.8843333333333333\n",
      "INFO:__main__:model saved to ./model\n",
      "INFO:__main__:epoch : 2\n",
      "INFO:__main__:train mode\n",
      "INFO:__main__:train_step : 1000, elapsed time[424.3601794242859]\n",
      "INFO:__main__:train_step : 2000, elapsed time[848.551353931427]\n",
      "INFO:__main__:train_step : 3000, elapsed time[1272.9303557872772]\n",
      "INFO:__main__:train_step : 4000, elapsed time[1697.019846200943]\n",
      "INFO:__main__:train_step : 5000, elapsed time[2121.117679834366]\n",
      "INFO:__main__:train_step : 6000, elapsed time[2545.061143875122]\n",
      "INFO:__main__:train_step : 7000, elapsed time[2969.33132314682]\n",
      "INFO:__main__:train_step : 8000, elapsed time[3393.4144818782806]\n",
      "INFO:__main__:train_step : 9000, elapsed time[3817.837587118149]\n",
      "INFO:__main__:train_step : 10000, elapsed time[4242.084632873535]\n",
      "INFO:__main__:train_step : 11000, elapsed time[4666.262772321701]\n",
      "INFO:__main__:train_step : 12000, elapsed time[5090.542105436325]\n",
      "INFO:__main__:train_loss : 0.2637685925423478\n",
      "INFO:__main__:eval mode\n",
      "INFO:__main__:eval loss : 0.301277593036355, acc : 0.8843333333333333\n",
      "INFO:__main__:model saved to ./model\n",
      "INFO:__main__:epoch : 3\n",
      "INFO:__main__:train mode\n",
      "INFO:__main__:train_step : 1000, elapsed time[424.1361117362976]\n",
      "INFO:__main__:train_step : 2000, elapsed time[848.2226028442383]\n",
      "INFO:__main__:train_step : 3000, elapsed time[1272.3603477478027]\n",
      "INFO:__main__:train_step : 4000, elapsed time[1696.5739047527313]\n",
      "INFO:__main__:train_step : 5000, elapsed time[2120.777046918869]\n",
      "INFO:__main__:train_step : 6000, elapsed time[2544.8765428066254]\n",
      "INFO:__main__:train_step : 7000, elapsed time[2969.1366913318634]\n",
      "INFO:__main__:train_step : 8000, elapsed time[3392.9185812473297]\n",
      "INFO:__main__:train_step : 9000, elapsed time[3817.1459078788757]\n",
      "INFO:__main__:train_step : 10000, elapsed time[4241.323096752167]\n",
      "INFO:__main__:train_step : 11000, elapsed time[4665.373409509659]\n",
      "INFO:__main__:train_step : 12000, elapsed time[5089.328632593155]\n",
      "INFO:__main__:train_loss : 0.26380756574853637\n",
      "INFO:__main__:eval mode\n",
      "INFO:__main__:eval loss : 0.301277593036355, acc : 0.8843333333333333\n",
      "INFO:__main__:model saved to ./model\n",
      "INFO:__main__:epoch : 4\n",
      "INFO:__main__:train mode\n",
      "INFO:__main__:train_step : 1000, elapsed time[423.9288606643677]\n",
      "INFO:__main__:train_step : 2000, elapsed time[847.8873081207275]\n",
      "INFO:__main__:train_step : 3000, elapsed time[1272.1211004257202]\n",
      "INFO:__main__:train_step : 4000, elapsed time[1696.2740404605865]\n",
      "INFO:__main__:train_step : 5000, elapsed time[2120.055630683899]\n",
      "INFO:__main__:train_step : 6000, elapsed time[2544.3310508728027]\n",
      "INFO:__main__:train_step : 7000, elapsed time[2968.8464069366455]\n",
      "INFO:__main__:train_step : 8000, elapsed time[3392.8975625038147]\n",
      "INFO:__main__:train_step : 9000, elapsed time[3816.779387950897]\n",
      "INFO:__main__:train_step : 10000, elapsed time[4240.801373004913]\n",
      "INFO:__main__:train_step : 11000, elapsed time[4665.173144340515]\n",
      "INFO:__main__:train_step : 12000, elapsed time[5089.396076440811]\n",
      "INFO:__main__:train_loss : 0.2628340887678787\n",
      "INFO:__main__:eval mode\n",
      "INFO:__main__:eval loss : 0.301277593036355, acc : 0.8843333333333333\n",
      "INFO:__main__:model saved to ./model\n"
     ]
    }
   ],
   "source": [
    "global_step=0\n",
    "total_tr_loss=0.0\n",
    "logging_steps=1000\n",
    "import time\n",
    "logger.info(f'num examples {len(train_dataset)}')\n",
    "logger.info(f'train batch size {train_batch_size}')\n",
    "logger.info(f'num of train epochs : {num_train_epochs}')\n",
    "logger.info(f'num of train steps each epoch : {len(train_dataloader)}')\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    logger.info('epoch : %s', epoch)\n",
    "\n",
    "    # train\n",
    "    logger.info('train mode')\n",
    "    model.train()\n",
    "    train_step=0\n",
    "    tr_loss=0.0\n",
    "    start_t=time.time()\n",
    "    model.zero_grad()\n",
    "    for step, (input_ids, attention_masks, token_type_ids, labels) in enumerate(train_dataloader):\n",
    "        input_ids=input_ids.to(device)\n",
    "        attention_masks=attention_masks.to(device)\n",
    "        token_type_ids=token_type_ids.to(device)\n",
    "        labels=labels.to(device)\n",
    "        outputs=model(input_ids=input_ids, attention_mask=attention_masks, token_type_ids=token_type_ids, labels=labels)\n",
    "\n",
    "        loss=outputs[0]\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        tr_loss += loss.item()\n",
    "\n",
    "        # every step\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        model.zero_grad()\n",
    "        global_step+=1\n",
    "        train_step+=1\n",
    "        if train_step % logging_steps ==0:\n",
    "            logger.info('train_step : %s, elapsed time[%s]', train_step, time.time()-start_t)\n",
    "    tr_loss=tr_loss/train_step\n",
    "    logger.info(f'train_loss : {tr_loss}')\n",
    "    # eval\n",
    "    logger.info('eval mode')\n",
    "    eval_loss=0.0\n",
    "    eval_step=0\n",
    "    out_label_ids=None\n",
    "    preds=None\n",
    "    model.eval()\n",
    "    for batch in dev_dataloader:\n",
    "        batch=tuple(t.to(device) for t in batch)\n",
    "        with torch.no_grad():\n",
    "            inputs = {\n",
    "                'input_ids': batch[0],\n",
    "                'attention_mask': batch[1],\n",
    "                'token_type_ids': batch[2],\n",
    "                'labels': batch[3]\n",
    "            }\n",
    "            tmp_eval_loss, logits =model(**inputs)\n",
    "            #logger.info(type(tmp_eval_loss))\n",
    "            eval_loss+=tmp_eval_loss.mean().item() #??\n",
    "        eval_step+=1\n",
    "        if preds is None:\n",
    "            preds = logits.detach().cpu().numpy()\n",
    "            out_label_ids = inputs['labels'].detach().cpu().numpy()\n",
    "        else:\n",
    "            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "            out_label_ids = np.append(out_label_ids, inputs['labels'].detach().cpu().numpy(), axis=0)\n",
    "    eval_loss=eval_loss/eval_step\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    acc=(preds==out_label_ids).mean()\n",
    "    logger.info(f'eval loss : {eval_loss}, acc : {acc}')\n",
    "    model_to_save=model.module if hasattr(model, 'module') else model\n",
    "    model_to_save.save_pretrained(model_save_path)\n",
    "    logger.info(f'model saved to {model_save_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "universal-thumbnail",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    2,  2047,  8167, 30738, 16545, 28382, 19973, 23059, 37460, 28382,\n",
       "        21422,     3,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ideal-translation",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sampler=SequentialSampler(test_dataset)\n",
    "test_dataloader=DataLoader(test_dataset, sampler=test_sampler, batch_size=test_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acting-ghost",
   "metadata": {},
   "source": [
    "### Testset Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "academic-infrared",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:__main__:test loss : 0.305619674174071, acc : 0.883\n"
     ]
    }
   ],
   "source": [
    "eval_loss=0.0\n",
    "eval_step=0\n",
    "out_label_ids=None\n",
    "preds=None\n",
    "model.eval()\n",
    "for batch in test_dataloader:\n",
    "    batch=tuple(t.to(device) for t in batch)\n",
    "    with torch.no_grad():\n",
    "        inputs = {\n",
    "            'input_ids': batch[0],\n",
    "            'attention_mask': batch[1],\n",
    "            'token_type_ids': batch[2],\n",
    "            'labels': batch[3]\n",
    "        }\n",
    "        tmp_eval_loss, logits =model(**inputs)\n",
    "#         logger.info(tmp_eval_loss.mean())\n",
    "        eval_loss+=tmp_eval_loss.mean().item() #??\n",
    "    eval_step+=1\n",
    "    if preds is None:\n",
    "        preds = logits.detach().cpu().numpy()\n",
    "        out_label_ids = inputs['labels'].detach().cpu().numpy()\n",
    "    else:\n",
    "        preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "        out_label_ids = np.append(out_label_ids, inputs['labels'].detach().cpu().numpy(), axis=0)\n",
    "eval_loss=eval_loss/eval_step\n",
    "preds = np.argmax(preds, axis=1)\n",
    "acc=(preds==out_label_ids).mean()\n",
    "logger.info(f'test loss : {eval_loss}, acc : {acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "respiratory-fruit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:__main__:(300, 2)\n",
      "INFO:__main__:(300,)\n",
      "INFO:__main__:eval loss : 0.5587934672832489, acc : 0.7333333333333333\n"
     ]
    }
   ],
   "source": [
    "eval_loss=eval_loss/eval_step\n",
    "logger.info(preds.shape)\n",
    "logger.info(out_label_ids.shape)\n",
    "preds = np.argmax(preds, axis=1)\n",
    "acc=(preds==out_label_ids).mean()\n",
    "logger.info(f'eval loss : {eval_loss}, acc : {acc}')\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earned-discrimination",
   "metadata": {},
   "source": [
    "### Test Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "laughing-realtor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:__main__:[[0.96186066 0.0381393 ]]\n"
     ]
    }
   ],
   "source": [
    "test_sentence=\"이 영화 정말 별로다\"\n",
    "\n",
    "tokens=tokenizer.tokenize(test_sentence)\n",
    "special_tokens_count = 2\n",
    "if len(tokens) > max_seq_len - special_tokens_count:\n",
    "    tokens = tokens[:(max_seq_len - special_tokens_count)]\n",
    "tokens=[tokenizer.cls_token]+tokens+[tokenizer.sep_token]\n",
    "token_type_ids=[0]*len(tokens)\n",
    "input_ids=tokenizer.convert_tokens_to_ids(tokens)\n",
    "attention_mask=[1]*len(input_ids)\n",
    "\n",
    "padding_length=max_seq_len-len(input_ids)\n",
    "\n",
    "input_ids= input_ids + ([tokenizer.pad_token_id]* padding_length)\n",
    "attention_mask = attention_mask + ([0] * padding_length)\n",
    "token_type_ids = token_type_ids + ([0] * padding_length)\n",
    "\n",
    "all_input_ids=torch.tensor([input_ids], dtype=torch.long)\n",
    "all_attention_mask=torch.tensor([attention_mask], dtype=torch.long)\n",
    "all_token_type_ids=torch.tensor([token_type_ids], dtype=torch.long)\n",
    "#evaluation\n",
    "encoding={\n",
    "    'input_ids': all_input_ids,\n",
    "    'attention_mask': all_attention_mask,\n",
    "    'token_type_ids': all_token_type_ids\n",
    "}\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    encoding_={key:t.to(device) for key,t in encoding.items()}\n",
    "    outputs=model(**encoding_)\n",
    "    logits=outputs[0]\n",
    "    preds=logits.detach().cpu().numpy()\n",
    "    \n",
    "from scipy.special import softmax\n",
    "preds=softmax(preds)\n",
    "logger.info(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedicated-breakfast",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens=tokenizer.tokenize(example[1])\n",
    "\n",
    "\n",
    "\n",
    "tokens=[cls_token]+tokens+[sep_token]\n",
    "token_type_ids=[0]*len(tokens)\n",
    "\n",
    "input_ids=tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "attention_mask=[1]*len(input_ids)\n",
    "\n",
    "padding_length=max_seq_len-len(input_ids)\n",
    "\n",
    "input_ids = input_ids + ([tokenizer.pad_token_id] * padding_length)\n",
    "attention_mask = attention_mask + ([0] * padding_length)\n",
    "token_type_ids = token_type_ids + ([0] * padding_length)\n",
    "features.append(InputFeatures(input_ids, attention_mask, token_type_ids, example[2]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
